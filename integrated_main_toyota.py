# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒˆãƒ¨ã‚¿ç‰ˆãƒ»ä¿®æ­£ç‰ˆï¼‰ï¼š
1. Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢çµæœã‹ã‚‰ã€Œãƒˆãƒ¨ã‚¿ã€ã®è¨˜äº‹ãƒªã‚¹ãƒˆã‚’å–å¾—ã€‚
2. ãã®ãƒªã‚¹ãƒˆã‚’å˜ä¸€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã€ŒYahooã€ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã€‚
3. è¿½è¨˜ã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰ã€å‰æ—¥15:00ã€œå½“æ—¥14:59:59ã®åˆ†ã‚’æŠ½å‡ºã—ã€
   è¨˜äº‹æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã€‚
4. å–å¾—ã—ãŸè©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’åŒã˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å½“æ—¥æ—¥ä»˜ã‚¿ãƒ–ã«æ›¸ãè¾¼ã¿ã€‚

ã€æœ¬ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆã€‘
- ã‚³ãƒ¡ãƒ³ãƒˆã¯ã€Œ1ãƒšãƒ¼ã‚¸ï¼æœ€å¤§10ã‚³ãƒ¡ãƒ³ãƒˆã€ã‚’JSONé…åˆ—æ–‡å­—åˆ—ã¨ã—ã¦1ã‚»ãƒ«ã«ã¾ã¨ã‚ã€ãƒšãƒ¼ã‚¸ã”ã¨ã« Q åˆ—ä»¥é™ã¸é…ç½®ã€‚
  ä¾‹ï¼‰Q: ã‚³ãƒ¡ãƒ³ãƒˆ(1ãƒšãƒ¼ã‚¸JSON), R: ã‚³ãƒ¡ãƒ³ãƒˆ(2ãƒšãƒ¼ã‚¸JSON), ...
- ãƒ˜ãƒƒãƒ€ãƒ¼ã‚‚ãƒšãƒ¼ã‚¸å˜ä½ã®JSONåˆ—åã¸å¤‰æ›´ã€‚
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ====== è¨­å®š ======
# æŒ‡å®šã•ã‚ŒãŸå˜ä¸€ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID (ãƒˆãƒ¨ã‚¿ç‰ˆ)
SHARED_SPREADSHEET_ID = "1AnpIQAxa-cVaPcB2nHc1cGkrmRCywjloUma05fs89Hs" 

# ã€ã‚¹ãƒ†ãƒƒãƒ—1, 2ã€‘ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—ç”¨ã®è¨­å®š
KEYWORD = "ãƒˆãƒ¨ã‚¿"
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"

# ã€ã‚¹ãƒ†ãƒƒãƒ—3, 4ã€‘æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã®å‡ºåŠ›å…ˆ
DEST_SPREADSHEET_ID = SHARED_SPREADSHEET_ID

# æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—è¨­å®š
MAX_BODY_PAGES = 10
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
MAX_TOTAL_COMMENTS = 5000

TZ_JST = timezone(timedelta(hours=9))

# ====== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj) -> str:
    """datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æŒ‡å®šã®å½¢å¼ã§æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¾ã™ (YYYY/MM/DD HH:MM)"""
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def format_yy_m_d_hm(dt: datetime) -> str:
    """yy/m/d HH:MM ã«æ•´å½¢ï¼ˆå…ˆé ­ã‚¼ãƒ­ã®æœˆæ—¥ã‚’é¿ã‘ã‚‹ï¼‰"""
    yy = dt.strftime("%y")
    m = str(int(dt.strftime("%m")))
    d = str(int(dt.strftime("%d")))
    hm = dt.strftime("%H:%M")
    return f"{yy}/{m}/{d} {hm}"

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    """
    ã‚½ãƒ¼ã‚¹Cåˆ—ï¼ˆæŠ•ç¨¿æ—¥ï¼‰ã‚’ JST datetime ã«å¤‰æ›
    è¨±å®¹: "MM/DD HH:MM"ï¼ˆå¹´ã¯å½“å¹´è£œå®Œï¼‰, "YYYY/MM/DD HH:MM", "YYYY/MM/DD HH:MM:SS", Excelã‚·ãƒªã‚¢ãƒ«
    """
    if raw is None: return None
    if isinstance(raw, str):
        s = raw.strip()
        for fmt in ("%m/%d %H:%M", "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    dt = dt.replace(year=today_jst.year)
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None
    if isinstance(raw, (int, float)):
        epoch = datetime(1899, 12, 30, tzinfo=TZ_JST)  # Excelèµ·ç‚¹
        return epoch + timedelta(days=float(raw))
    if isinstance(raw, datetime):
        return raw.astimezone(TZ_JST) if raw.tzinfo else raw.replace(tzinfo=TZ_JST)
    return None

def chunk(lst: List[str], size: int) -> List[List[str]]:
    """ãƒªã‚¹ãƒˆã‚’ size ä»¶ã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆsize <= 0 ã®å ´åˆã¯å…¨ä½“ã‚’1ãƒãƒ£ãƒ³ã‚¯ï¼‰"""
    if size <= 0:
        return [lst]
    return [lst[i:i + size] for i in range(0, len(lst), size)]

# ====== èªè¨¼ ======

def build_gspread_client() -> gspread.Client:
    """
    gspreadã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    ç’°å¢ƒå¤‰æ•° GOOGLE_CREDENTIALS ã¾ãŸã¯ GCP_SERVICE_ACCOUNT_KEY ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    try:
        creds_str = os.environ.get("GOOGLE_CREDENTIALS")
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            creds_str_alt = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
            if creds_str_alt:
                credentials = json.loads(creds_str_alt)
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã¯ 'credentials.json' ãŒå¿…è¦
                credentials = json.load(open('credentials.json'))
                
            return gspread.service_account_from_dict(credentials)
            
    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")


# =========================================================================
# ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—
# =========================================================================

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ç”¨ã—ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    print("ğŸš€ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹...")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1280,1024")

    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"âŒ WebDriverã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
        
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    time.sleep(5) 

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    articles_data = []

    for article in articles:
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            
            # URL
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag else ""
            
            # æŠ•ç¨¿æ—¥
            time_tag = article.find("time")
            date_str = time_tag.text.strip() if time_tag else ""
            formatted_date = ""
            if date_str:
                date_str = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    dt_obj = datetime.strptime(date_str, "%Y/%m/%d %H:%M")
                    formatted_date = format_datetime(dt_obj)
                except:
                    formatted_date = date_str

            # å¼•ç”¨å…ƒï¼ˆã‚½ãƒ¼ã‚¹ï¼‰
            source_text = ""
            source_tag = article.find("div", class_="sc-n3vj8g-0 yoLqH")
            if source_tag:
                inner = source_tag.find("div", class_="sc-110wjhy-8 bsEjY")
                if inner and inner.span:
                    candidate = inner.span.text.strip()
                    if not candidate.isdigit():
                        source_text = candidate
            if not source_text or source_text.isdigit():
                alt_spans = article.find_all(["span", "div"], string=True)
                for s in alt_spans:
                    text = s.text.strip()
                    if 2 <= len(text) <= 20 and not text.isdigit() and re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥A-Za-z]', text):
                        source_text = text
                        break

            if title and url:
                articles_data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": formatted_date if formatted_date else "å–å¾—ä¸å¯",
                    "å¼•ç”¨å…ƒ": source_text
                })
        except Exception:
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

def write_news_list_to_source(gc: gspread.Client, articles: list[dict]):
    """
    ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘å–å¾—ã—ãŸè¨˜äº‹ãƒªã‚¹ãƒˆã‚’SOURCE_SPREADSHEETã®'Yahoo'ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã™ã€‚
    """
    for attempt in range(5):
        try:
            sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
            try:
                worksheet = sh.worksheet(SOURCE_SHEET_NAME)
            except gspread.exceptions.WorksheetNotFound:
                worksheet = sh.add_worksheet(title=SOURCE_SHEET_NAME, rows="1", cols="4")
                worksheet.append_row(['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])

            existing_data = worksheet.get_all_values()
            existing_urls = set(row[1] for row in existing_data[1:] if len(row) > 1)

            # A:ã‚¿ã‚¤ãƒˆãƒ« / B:URL / C:æŠ•ç¨¿æ—¥ / D:å¼•ç”¨å…ƒ ã®å½¢å¼ã§æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            new_data = [[a['ã‚¿ã‚¤ãƒˆãƒ«'], a['URL'], a['æŠ•ç¨¿æ—¥'], a['å¼•ç”¨å…ƒ']] for a in articles if a['URL'] not in existing_urls]
            
            if new_data:
                worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
                print(f"âœ… SOURCEã‚·ãƒ¼ãƒˆã« {len(new_data)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            else:
                print("âš ï¸ SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
            
        except gspread.exceptions.APIError as e:
            print(f"âš ï¸ Google API Error (attempt {attempt + 1}/5): {e}")
            time.sleep(5 + random.random() * 5)

    raise RuntimeError("âŒ SOURCEã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ5å›è©¦è¡Œã—ã¦ã‚‚æˆåŠŸã›ãšï¼‰")


# =========================================================================
# ã€ã‚¹ãƒ†ãƒƒãƒ—3, 4ã€‘ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—
# =========================================================================

# --- DESTã‚·ãƒ¼ãƒˆæ“ä½œ ---
def ensure_today_sheet(sh: gspread.Spreadsheet, today_tab: str) -> gspread.Worksheet:
    """å½“æ—¥ã‚¿ãƒ–ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆã—ã¾ã™ï¼ˆå…ƒè¨­å®šã®ã¾ã¾ rows=3000, cols=300ï¼‰"""
    try:
        ws = sh.worksheet(today_tab)
    except gspread.WorksheetNotFound:
      ws = sh.add_worksheet(title=today_tab, rows="500", cols="516")
    return ws

def get_existing_urls(ws: gspread.Worksheet) -> Set[str]:
    """DESTã‚·ãƒ¼ãƒˆã®Cåˆ—ï¼ˆURLï¼‰ã‹ã‚‰æ—¢å­˜URLã‚’å–å¾—"""
    vals = ws.col_values(3)
    return set(vals[1:] if len(vals) > 1 else [])

def ensure_ae_header(ws: gspread.Worksheet) -> None:
    """Aã€œEåˆ—ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä¿è¨¼ï¼ˆgspreadã®è­¦å‘Šã‚’å›é¿æ¸ˆã¿ï¼‰"""
    head = ws.row_values(1)
    target = ["ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "æ²è¼‰å…ƒ"]
    if head[:len(target)] != target:
        ws.update(range_name='A1', values=[target])

def ensure_body_comment_headers(ws: gspread.Worksheet, max_comment_pages: int) -> None:
    """
    Fåˆ—ä»¥é™ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä¿è¨¼ï¼š
      - F..O: æœ¬æ–‡(1ã€œ10ãƒšãƒ¼ã‚¸)
      - P: ã‚³ãƒ¡ãƒ³ãƒˆæ•°
      - Q..: ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆãƒšãƒ¼ã‚¸å˜ä½JSONï¼‰ä¾‹: ã‚³ãƒ¡ãƒ³ãƒˆ(1ãƒšãƒ¼ã‚¸JSON), ã‚³ãƒ¡ãƒ³ãƒˆ(2ãƒšãƒ¼ã‚¸JSON)...
    """
    base = ["ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "æ²è¼‰å…ƒ"]
    body_headers = [f"æœ¬æ–‡({i}ãƒšãƒ¼ã‚¸)" for i in range(1, 11)]
    comments_count = ["ã‚³ãƒ¡ãƒ³ãƒˆæ•°"]
    comment_headers = [f"ã‚³ãƒ¡ãƒ³ãƒˆ({i}ãƒšãƒ¼ã‚¸JSON)" for i in range(1, max(1, max_comment_pages) + 1)]
    target = base + body_headers + comments_count + comment_headers

    current = ws.row_values(1)
    if current != target:
        ws.update(range_name='A1', values=[target])


# --- ãƒ‡ãƒ¼ã‚¿è»¢é€ ---
def transfer_a_to_e(gc: gspread.Client, dest_ws: gspread.Worksheet) -> int:
    """
    SOURCEã‚·ãƒ¼ãƒˆã‹ã‚‰ã€Œå‰æ—¥15:00ã€œå½“æ—¥14:59:59ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’DESTã‚·ãƒ¼ãƒˆã®Aã€œEåˆ—ã«è»¢é€
    """
    sh_src = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    ws_src = sh_src.worksheet(SOURCE_SHEET_NAME)
    rows = ws_src.get('A:D')

    now = jst_now()
    # å‰æ—¥15:00:00 JST
    start = (now - timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
    # å½“æ—¥14:59:59 JST
    end = now.replace(hour=14, minute=59, second=59, microsecond=0)

    ensure_ae_header(dest_ws)
    existing = get_existing_urls(dest_ws)

    to_append: List[List[str]] = []
    for i, r in enumerate(rows):
        if i == 0: continue
        title = r[0].strip() if len(r) > 0 and r[0] else ""
        url = r[1].strip() if len(r) > 1 and r[1] else ""
        posted_raw = r[2] if len(r) > 2 else ""
        site = r[3].strip() if len(r) > 3 and r[3] else ""
        if not title or not url: continue
        
        dt = parse_post_date(posted_raw, now)
        if not dt or not (start <= dt <= end):
            continue # æ™‚é–“ç¯„å›²å¤–
            
        if url in existing:
            continue # DESTã‚·ãƒ¼ãƒˆã«é‡è¤‡

        # A:ã‚½ãƒ¼ã‚¹ / B:ã‚¿ã‚¤ãƒˆãƒ« / C:URL / D:æŠ•ç¨¿æ—¥ / E:æ²è¼‰å…ƒ
        to_append.append(["Yahoo", title, url, format_yy_m_d_hm(dt), site])

    if to_append:
        dest_ws.append_rows(to_append, value_input_option="USER_ENTERED")
    return len(to_append)


# --- æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ---
def fetch_article_pages(base_url: str) -> Tuple[str, str, List[str]]:
    """è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ã—ã¾ã™"""
    title = "å–å¾—ä¸å¯"
    article_date = "å–å¾—ä¸å¯"
    bodies: List[str] = []
    for page in range(1, MAX_BODY_PAGES + 1):
        url = base_url if page == 1 else f"{base_url}?page={page}"
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            res.raise_for_status()
        except Exception:
            break
        soup = BeautifulSoup(res.text, "html.parser")
        
        if page == 1:
            t = soup.find("title")
            if t and t.get_text(strip=True):
                title = t.get_text(strip=True).replace(" - Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹", "")
            time_tag = soup.find("time")
            if time_tag:
                article_date = time_tag.get_text(strip=True)

        body_text = ""
        article = soup.find("article")
        if article:
            ps = article.find_all("p")
            body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
        
        if not body_text:
            main = soup.find("main")
            if main:
                ps = main.find_all("p")
                body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))

        if not body_text or (bodies and body_text == bodies[-1]):
            break
        bodies.append(body_text)
    return title, article_date, bodies

def fetch_comments_with_selenium(base_url: str) -> List[str]:
    """è¨˜äº‹ã‚³ãƒ¡ãƒ³ãƒˆã‚’Seleniumã§å–å¾—ã—ã¾ã™ï¼ˆå…¨ãƒšãƒ¼ã‚¸å·¡å›ãƒ»ä¸Šé™ã‚ã‚Šï¼‰"""
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2000")
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"âŒ WebDriveråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    comments: List[str] = []
    last_tail: Optional[str] = None
    page = 1
    try:
        while True:
            c_url = f"{base_url}/comments?page={page}"
            driver.get(c_url)
            time.sleep(2.0)

            soup = BeautifulSoup(driver.page_source, "html.parser")

            selectors = [
                "p.sc-169yn8p-10",
                "p[data-ylk*='cm_body']",
                "p[class*='comment']",
                "div.commentBody, p.commentBody",
                "div[data-ylk*='cm_body']"
            ]

            p_candidates = []
            for sel in selectors:
                p_candidates.extend(soup.select(sel))

            page_comments = [p.get_text(strip=True) for p in p_candidates if p.get_text(strip=True)]
            page_comments = list(dict.fromkeys(page_comments))

            if not page_comments: break

            if last_tail is not None and page_comments and page_comments[0] == last_tail: break

            comments.extend(page_comments)

            if len(comments) >= MAX_TOTAL_COMMENTS:
                comments = comments[:MAX_TOTAL_COMMENTS]
                break

            last_tail = page_comments[-1]
            page += 1

    finally:
        driver.quit()

    return comments

def write_bodies_and_comments(ws: gspread.Worksheet) -> None:
    """
    DESTã‚·ãƒ¼ãƒˆã®Fåˆ—ä»¥é™ã«æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ãè¾¼ã¿
      - æœ¬æ–‡: F..Oï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰
      - ã‚³ãƒ¡ãƒ³ãƒˆæ•°: P
      - ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡: Q..ï¼ˆ1ãƒšãƒ¼ã‚¸=æœ€å¤§10ã‚³ãƒ¡ãƒ³ãƒˆã‚’JSONæ–‡å­—åˆ—ã§1ã‚»ãƒ«ï¼‰
    """
    urls = ws.col_values(3)[1:]
    total = len(urls)
    print(f"ğŸ” æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—å¯¾è±¡URL: {total} ä»¶")
    if total == 0: return

    rows_data: List[List[str]] = []
    max_comment_pages = 0  # æœ€å¤§å…¨ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã¨åˆ—å¹…ã®æŒ‡æ¨™ï¼‰

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®è¡Œç•ªå·(2ã‹ã‚‰é–‹å§‹)
    for row_idx, url in enumerate(urls, start=2):
        print(f"  - ({row_idx-1}/{total}) {url}")
        try:
            _title, _date, bodies = fetch_article_pages(url)
            comments = fetch_comments_with_selenium(url)

            # æœ¬æ–‡ã‚»ãƒ«ï¼ˆæœ€å¤§ MAX_BODY_PAGES ã«ãƒ•ã‚£ãƒƒãƒˆï¼‰
            body_cells = bodies[:MAX_BODY_PAGES] + [""] * (MAX_BODY_PAGES - len(bodies))

            # ã‚³ãƒ¡ãƒ³ãƒˆã¯10ä»¶ï¼1ãƒšãƒ¼ã‚¸ã¨ã—ã¦åˆ†å‰²ã—ã€ãƒšãƒ¼ã‚¸ã”ã¨ã«JSONæ–‡å­—åˆ—ã¸
            comment_pages = chunk(comments, 10)  # [[c1..c10], [c11..c20], ...]
            json_per_page = [json.dumps(pg, ensure_ascii=False) for pg in comment_pages]
            cnt = len(comments)

            # è¡Œãƒ‡ãƒ¼ã‚¿: [æœ¬æ–‡x10, ã‚³ãƒ¡ãƒ³ãƒˆæ•°, ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸JSONxN]
            row = body_cells + [cnt] + json_per_page
            rows_data.append(row)

            if len(comment_pages) > max_comment_pages:
                max_comment_pages = len(comment_pages)

        except Exception as e:
            print(f"    ! Error: {e}")
            rows_data.append(([""] * MAX_BODY_PAGES) + [0])  # ã‚³ãƒ¡ãƒ³ãƒˆJSONãªã—

    # ãƒ˜ãƒƒãƒ€ãƒ¼æ•´å‚™ï¼ˆãƒšãƒ¼ã‚¸æ•°ã«å¿œã˜ã¦å¯å¤‰åˆ—åã‚’ä½œæˆï¼‰
    ensure_body_comment_headers(ws, max_comment_pages=max_comment_pages)

    # å„è¡Œã®é•·ã•ã‚’ã€Œæœ¬æ–‡(10)+ã‚³ãƒ¡ãƒ³ãƒˆæ•°(1)+max_comment_pagesã€ã«åˆã‚ã›ã¦å³å´ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    need_cols = MAX_BODY_PAGES + 1 + max_comment_pages
    for i in range(len(rows_data)):
        if len(rows_data[i]) < need_cols:
            rows_data[i].extend([""] * (need_cols - len(rows_data[i])))

    # F2 ã‹ã‚‰ä¸€æ‹¬æ›´æ–°
    if rows_data:
        ws.update(range_name="F2", values=rows_data)
        print(f"âœ… Fåˆ—ä»¥é™ã«æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆ(JSON/ãƒšãƒ¼ã‚¸å˜ä½)ã‚’æ›¸ãè¾¼ã¿å®Œäº†: {len(rows_data)} è¡Œï¼ˆæœ€å¤§ãƒšãƒ¼ã‚¸={max_comment_pages}ï¼‰")


# =========================================================================
# ã€ãƒ¡ã‚¤ãƒ³å‡¦ç†ã€‘
# =========================================================================

def main():
    gc = build_gspread_client()
    
    # 1. Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’å–å¾—
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    
    if not yahoo_news_articles:
        print("ğŸ’¡ æ–°ã—ã„è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    # 2. ãƒªã‚¹ãƒˆã‚’ä¸€æ™‚çš„ãªSOURCEã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
    print(f"\nğŸ“„ Spreadsheet ID: {SHARED_SPREADSHEET_ID} / Sheet: {SOURCE_SHEET_NAME}")
    write_news_list_to_source(gc, yahoo_news_articles)
    
    # 3. DESTã‚·ãƒ¼ãƒˆã‚’æº–å‚™
    dest_sh = gc.open_by_key(DEST_SPREADSHEET_ID)
    today_tab = jst_now().strftime("%y%m%d") # yymmdd å½¢å¼ã®ã‚¿ãƒ–å
    ws = ensure_today_sheet(dest_sh, today_tab)
    print(f"\nğŸ“„ DEST Sheet: {today_tab}")

    # 4. SOURCEã‹ã‚‰DESTã¸ã€Œå‰æ—¥15:00ã€œå½“æ—¥14:59:59ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ (Aã€œEåˆ—)
    added = transfer_a_to_e(gc, ws)
    print(f"ğŸ“ DESTã‚·ãƒ¼ãƒˆã«æ–°è¦è¿½åŠ : {added} è¡Œ")
    
    # 5. DESTã‚·ãƒ¼ãƒˆã®è¨˜äº‹ã«å¯¾ã—ã¦æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾— (Fåˆ—ä»¥é™)
    if ws.get_all_values(value_render_option='UNFORMATTED_VALUE'):
        write_bodies_and_comments(ws)
    else:
        print("âš ï¸ å½“æ—¥ã‚·ãƒ¼ãƒˆã«è¡ŒãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


if __name__ == "__main__":
    main()
